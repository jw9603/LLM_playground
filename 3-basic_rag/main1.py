import streamlit as st
from dotenv import load_dotenv

from langchain_teddynote import logging
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.document_loaders import PyMuPDFLoader
from langchain_community.vectorstores import FAISS
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough
from langchain_core.prompts import PromptTemplate
from langchain_openai import ChatOpenAI, OpenAIEmbeddings

import os
import tempfile

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()
logging.langsmith('Basic-RAG-Streamlit')

st.set_page_config(page_title="PDF ê¸°ë°˜ Q&A", page_icon="ğŸ“„")
st.title("ğŸ“„ PDF ê¸°ë°˜ Q&A ì„œë¹„ìŠ¤")

# 1. PDF ì—…ë¡œë“œ
uploaded_file = st.file_uploader("PDF íŒŒì¼ì„ ì—…ë¡œë“œ í•´ì£¼ì„¸ìš”", type=["pdf"])

if uploaded_file:
    # 1-1. ì„ì‹œ íŒŒì¼ë¡œ ì €ì¥
    with tempfile.NamedTemporaryFile(delete=False, suffix=".pdf") as tmp_file:
        tmp_file.write(uploaded_file.read())
        tmp_pdf_path = tmp_file.name

    # 2. ë¬¸ì„œ ë¡œë“œ
    loader = PyMuPDFLoader(tmp_pdf_path)
    docs = loader.load()

    # 3. ë¬¸ì„œ ë¶„í• 
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)
    split_documents = text_splitter.split_documents(docs)

    # 4. ì„ë² ë”© ìƒì„±
    embeddings = OpenAIEmbeddings()

    # 5. ë²¡í„° DB ìƒì„±
    vector_store = FAISS.from_documents(documents=split_documents, embedding=embeddings)

    # 6. Retriever ìƒì„±
    retriever = vector_store.as_retriever()

    # 7. í”„ë¡¬í”„íŠ¸ ìƒì„±
    prompt = PromptTemplate.from_template(
        """You are an assistant for question-answering tasks.
    You are given some context documents. Use them to help answer the question if relevant.
    If the context is not helpful, feel free to answer based on your own knowledge.
    Always include "page" number if the information is from context.
    Please Answer in Korean.
    
    #Context:
    {context}
    
    #Question:
    {question}
    
    #Answer: """
    )

    # 8. LLM + ì²´ì¸ ìƒì„±
    llm = ChatOpenAI(model_name='gpt-4o', temperature=0)
    chain = (
        {'context': retriever, "question": RunnablePassthrough()}
        | prompt
        | llm
        | StrOutputParser()
    )

    # 9. ì‚¬ìš©ì ì§ˆë¬¸ ì…ë ¥ ë°›ê¸°
    question = st.text_input("â“ ê¶ê¸ˆí•œ ë‚´ìš©ì„ ì…ë ¥í•˜ì„¸ìš”")

    # 10. ì§ˆë¬¸ ì…ë ¥ ì‹œ ì‘ë‹µ ì¶œë ¥
    if question:
        st.markdown("### ğŸ¤– ë‹µë³€")
        response_stream = chain.stream(question)

        # ì‹¤ì‹œê°„ ì‘ë‹µ ì¶œë ¥
        output_container = st.empty()
        full_response = ""
        for chunk in response_stream:
            full_response += chunk
            output_container.markdown(full_response)

else:
    st.info("ë¨¼ì € PDF íŒŒì¼ì„ ì—…ë¡œë“œí•´ì£¼ì„¸ìš”.")